"""
å°†æœ¬åœ°æ–‡ä»¶ï¼ˆdata/ï¼‰åˆ‡å— -> è°ƒç”¨ OpenAI Embeddings -> å†™å…¥ Chroma æŒä¹…åŒ–å‘é‡åº“ã€‚
è¿è¡Œç¤ºä¾‹ï¼š
    python src/ingest.py --source data --persist .chroma --collection docs
"""

import os
import glob
import json
import argparse
from typing import List, Dict, Tuple

from dotenv import load_dotenv
load_dotenv()

# ---- OpenAI Embeddingsï¼ˆç›´æ¥ç”¨å®˜æ–¹ SDKï¼Œä¸ä¾èµ– langchainï¼‰----
from openai import OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("æœªæ‰¾åˆ° OPENAI_API_KEYã€‚è¯·åœ¨é¡¹ç›®æ ¹ç›®å½• .env å†™å…¥ï¼šOPENAI_API_KEY=sk-xxxx")
client = OpenAI(api_key=OPENAI_API_KEY)

EMBED_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")

# ---- Chroma æŒä¹…åŒ–å®¢æˆ·ç«¯ ----
import chromadb
from chromadb.config import Settings


# ========== I/O ==========

SUPPORTED_EXTS = {".txt", ".md", ".mdx", ".csv", ".json"}

def read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def read_pdf_file(path: str) -> str:
    """å¯é€‰ï¼šå®‰è£… pypdf åå¯ç”¨ PDF è¯»å–ã€‚"""
    try:
        from pypdf import PdfReader  # pip install pypdf
    except Exception as e:
        raise RuntimeError("è¯»å– PDF éœ€è¦å®‰è£… pypdfï¼špip install pypdf") from e
    reader = PdfReader(path)
    texts = []
    for page in reader.pages:
        texts.append(page.extract_text() or "")
    return "\n".join(texts)

def load_raw_documents(source_dir: str, pattern: str) -> List[Tuple[str, str]]:
    """
    è¿”å› [(path, text), ...]
    """
    files = glob.glob(os.path.join(source_dir, pattern), recursive=True)
    result = []
    for fp in files:
        ext = os.path.splitext(fp)[1].lower()
        try:
            if ext == ".pdf":
                text = read_pdf_file(fp)
            elif ext in SUPPORTED_EXTS:
                text = read_text_file(fp)
            else:
                # ä¸æ”¯æŒçš„åç¼€ç›´æ¥è·³è¿‡
                continue
            text = text.strip()
            if text:
                result.append((fp, text))
        except Exception as e:
            print(f"âš ï¸ è¯»å–å¤±è´¥ï¼š{fp} -> {e}")
    return result


# ========== æ–‡æœ¬åˆ‡å— ==========

def chunk_text(text: str, chunk_size: int = 800, chunk_overlap: int = 100) -> List[str]:
    """
    ç®€å•æŒ‰å­—ç¬¦åˆ‡å—ï¼Œå¸¦é‡å ã€‚ç”Ÿäº§ç¯å¢ƒå¯æ›¿æ¢ä¸ºæ›´èªæ˜çš„åˆ†å¥åˆ‡å—ã€‚
    """
    text = " ".join(text.split())  # å‹ç¼©å¤šç©ºç™½
    if chunk_size <= 0:
        return [text]
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(start + chunk_size, n)
        chunks.append(text[start:end])
        start = end - chunk_overlap
        if start < 0:
            start = 0
        if start >= n:
            break
    return [c for c in chunks if c.strip()]


# ========== Embeddings ==========

def embed_batch(texts: List[str]) -> List[List[float]]:
    """
    è°ƒç”¨ OpenAI embeddingsã€‚ä¸€æ¬¡æœ€å¤šè¾“å…¥ 2048 æ¡å·¦å³ï¼ˆå®˜æ–¹é™åˆ¶ä¼šå˜åŒ–ï¼›è¿™é‡Œåšåˆ†æ‰¹æ›´ç¨³ï¼‰ã€‚
    """
    out: List[List[float]] = []
    BATCH = 256
    for i in range(0, len(texts), BATCH):
        batch = texts[i:i+BATCH]
        resp = client.embeddings.create(model=EMBED_MODEL, input=batch)
        out.extend([d.embedding for d in resp.data])
    return out


# ========== å†™å…¥ Chroma ==========

def get_chroma_collection(persist_dir: str, collection_name: str):
    os.makedirs(persist_dir, exist_ok=True)
    client = chromadb.PersistentClient(
        path=persist_dir,
        settings=Settings(allow_reset=False)
    )
    coll = client.get_or_create_collection(collection_name)
    return coll

def upsert_documents(
    coll,
    docs: List[Tuple[str, str]],
    chunk_size: int,
    chunk_overlap: int
):
    """
    å°†æ–‡æ¡£åˆ‡å—åå†™å…¥/æ›´æ–°åˆ° Chromaã€‚
    id è§„åˆ™ï¼š {path}::{idx}
    metadata: {"source": path, "chunk": idx}
    """
    all_ids: List[str] = []
    all_docs: List[str] = []
    all_metas: List[Dict] = []

    for path, text in docs:
        chunks = chunk_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        for idx, ck in enumerate(chunks):
            all_ids.append(f"{path}::{idx}")
            all_docs.append(ck)
            all_metas.append({"source": path, "chunk": idx})

    if not all_docs:
        print("â„¹ï¸ æ²¡æœ‰å¯å†™å…¥çš„æ–‡æ¡£å—ã€‚")
        return 0

    print(f"ğŸ§© å…± {len(all_docs)} ä¸ªæ–‡æœ¬å—ï¼Œå¼€å§‹ç”Ÿæˆå‘é‡ï¼ˆæ¨¡å‹ï¼š{EMBED_MODEL}ï¼‰...")
    vectors = embed_batch(all_docs)
    print("âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œå†™å…¥ Chroma ...")

    # Chroma ä¼šæ ¹æ® id å»é‡/æ›´æ–°
    coll.upsert(
        ids=all_ids,
        documents=all_docs,
        metadatas=all_metas,
        embeddings=vectors
    )
    print("ğŸ‰ å†™å…¥å®Œæˆã€‚")
    return len(all_docs)


# ========== CLI ==========

def main():
    parser = argparse.ArgumentParser(description="å°†æœ¬åœ°æ–‡ä»¶å‘é‡åŒ–å¹¶å†™å…¥ Chroma")
    parser.add_argument("--source", default="data", help="åŸå§‹æ–‡æ¡£ç›®å½•ï¼ˆé»˜è®¤ dataï¼‰")
    parser.add_argument(
        "--pattern",
        default="**/*.*",
        help="æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼ˆglobï¼‰ï¼Œé»˜è®¤ '**/*.*'ï¼›æ”¯æŒ .txt .md .mdx .csv .json .pdf"
    )
    parser.add_argument("--persist", default=".chroma", help="Chroma æŒä¹…åŒ–ç›®å½•ï¼ˆé»˜è®¤ .chromaï¼‰")
    parser.add_argument("--collection", default="docs", help="é›†åˆåç§°ï¼ˆé»˜è®¤ docsï¼‰")
    parser.add_argument("--chunk-size", type=int, default=800, help="åˆ‡å—å¤§å°ï¼ˆé»˜è®¤ 800ï¼‰")
    parser.add_argument("--chunk-overlap", type=int, default=100, help="åˆ‡å—é‡å ï¼ˆé»˜è®¤ 100ï¼‰")
    args = parser.parse_args()

    print(f"ğŸ“‚ æ‰«æç›®å½•ï¼š{args.source}ï¼ˆæ¨¡å¼ï¼š{args.pattern}ï¼‰")
    raw_docs = load_raw_documents(args.source, args.pattern)
    print(f"ğŸ“ è¯»å–åˆ° {len(raw_docs)} ä¸ªæ–‡ä»¶ã€‚")

    coll = get_chroma_collection(args.persist, args.collection)
    n = upsert_documents(coll, raw_docs, args.chunk_size, args.chunk_overlap)

    # è®°å½•ä¸€æ¬¡ ingest ä¿¡æ¯
    meta = {
        "source": os.path.abspath(args.source),
        "pattern": args.pattern,
        "collection": args.collection,
        "persist": os.path.abspath(args.persist),
        "chunks_written": n,
        "embed_model": EMBED_MODEL,
    }
    os.makedirs(args.persist, exist_ok=True)
    with open(os.path.join(args.persist, "ingest_meta.json"), "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print(f"ğŸ§¾ å…ƒæ•°æ®å·²å†™å…¥ï¼š{args.persist}/ingest_meta.json")


if __name__ == "__main__":
    main()