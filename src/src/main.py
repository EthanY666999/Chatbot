# src/main.py
from .config import OPENAI_API_KEY, CHAT_MODEL, VECTOR_DB_PATH
from .prompt import SYSTEM_PROMPT, DEVELOPER_HINT
from .memory import VectorMemory, ChatMemory
import re
from openai import OpenAI
from .config import OPENAI_API_KEY, CHAT_MODEL, VECTOR_DB_PATH
from .prompt import SYSTEM_PROMPT, DEVELOPER_HINT
from .memory import VectorMemory, ChatMemory

# å„é›†åˆ
DOCS_COLLECTION   = "docs"        # å¤–éƒ¨æ–‡æ¡£ï¼ˆingestå†™å…¥ï¼‰
FACTS_COLLECTION  = "long_term"   # é•¿æœŸäº‹å®ï¼ˆå¯é€‰ï¼‰
NOTES_COLLECTION  = "user_notes"  # âœ¨ æ–°å¢ï¼šç”¨æˆ·å‘½åè®°å¿†
PERSIST_DIR       = VECTOR_DB_PATH

# ---------- å·¥å…·å‡½æ•° ----------
def build_recalled_context(recalls, min_score=0.2, max_items=5, label=""):
    seen, kept = set(), []
    for r in sorted(recalls, key=lambda x: x["score"], reverse=True):
        if r["score"] < min_score: continue
        key = r["text"].strip()[:100]
        if key in seen: continue
        seen.add(key); kept.append(r)
        if len(kept) >= max_items: break
    lines = []
    for i, r in enumerate(kept, 1):
        src = r.get("meta", {}).get("source") or r.get("meta", {}).get("name") or r.get("meta", {}).get("path") or ""
        src_info = f"  ({src})" if src else ""
        tag = f"[{label}]" if label else ""
        lines.append(f"[R{i}{tag}] {r['text']}{src_info}")
    return kept, "\n".join(lines)

def query_all(v_docs, v_facts, v_notes, q: str, k_each=6, min_score=0.2):
    docs_hits  = v_docs.query(q,  k=k_each) if v_docs  else []
    facts_hits = v_facts.query(q, k=k_each) if v_facts else []
    notes_hits = v_notes.query(q, k=k_each) if v_notes else []

    kd, bd = build_recalled_context(docs_hits,  min_score, 5, "docs")
    kf, bf = build_recalled_context(facts_hits, min_score, 3, "facts")
    kn, bn = build_recalled_context(notes_hits, min_score, 5, "note")

    blocks = [b for b in (bd, bf, bn) if b]
    return kd, kf, kn, "\n".join(blocks)

def extract_saveas(cmd: str):
    # å½¢å¦‚ï¼šsaveas è®°å¿†å: å†…å®¹
    m = re.match(r"^saveas\s+([^\:]+?)\s*:\s*(.+)$", cmd, flags=re.I)
    return (m.group(1).strip(), m.group(2).strip()) if m else (None, None)

# ---------- ä¸»ç¨‹åº ----------
def main():
    client = OpenAI(api_key=OPENAI_API_KEY)

    memory     = ChatMemory(max_turns=10)
    vmem_docs  = VectorMemory(persist_dir=PERSIST_DIR, collection=DOCS_COLLECTION)
    vmem_facts = VectorMemory(persist_dir=PERSIST_DIR, collection=FACTS_COLLECTION)
    vmem_notes = VectorMemory(persist_dir=PERSIST_DIR, collection=NOTES_COLLECTION)  # æ–°å¢é›†åˆ

    print("ğŸ¤– Chatbot å·²å¯åŠ¨ï¼Œè¾“å…¥ 'exit' é€€å‡ºã€‚")
    print("å‘½ä»¤ï¼š\n"
          "  rag? å…³é”®è¯        æŸ¥çœ‹ RAG å‘½ä¸­\n"
          "  save åç§°          ä¿å­˜æœ€è¿‘å¯¹è¯æ‘˜è¦ä¸ºå‘½åè®°å¿†\n"
          "  saveas åç§°: å†…å®¹  ç›´æ¥æŠŠæŒ‡å®šå†…å®¹ä¿å­˜ä¸ºè®°å¿†\n"
          "  recall åç§°/å…³é”®è¯  å¬å›è®°å¿†å¹¶æ³¨å…¥ä¸Šä¸‹æ–‡\n"
          "  list memories      åˆ—å‡ºæ‰€æœ‰å‘½åè®°å¿†ï¼ˆTop 20ï¼‰\n"
          "  delete åç§°        åˆ é™¤æœ€ç›¸å…³çš„ä¸€æ¡å‘½åè®°å¿†\n")

    while True:
        user_input = input("ä½ ï¼š").strip()
        if user_input.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ å†è§ï¼"); break

        # ---------- è°ƒè¯•/ç®¡ç†å‘½ä»¤ ----------
        # 1) æŸ¥çœ‹ RAG å‘½ä¸­
        if user_input.lower().startswith("rag?"):
            q = user_input.split("?", 1)[1].strip() or " "
            _, _, _, block = query_all(vmem_docs, vmem_facts, vmem_notes, q, k_each=8, min_score=0.0)
            print("\nğŸ” RAG å‘½ä¸­ï¼š\n" + (block or "(æ— æ£€ç´¢ç»“æœ)")); continue

        # 2) saveas åç§°: å†…å®¹
        name, content = extract_saveas(user_input)
        if name and content:
            vmem_notes.add_memories([content], [{"type":"user_note", "name": name}])
            print(f"âœ… å·²ä¿å­˜è®°å¿†ï¼š{name}"); continue

        if re.match(r"^save\d*\s", user_input.lower()):
        # åŒ¹é… save / save2 / save3 / save10 ...
            m = re.match(r"^save(\d*)\s+(.+)$", user_input.strip(), flags=re.I)
            count_str, name = m.groups()
            max_msgs = int(count_str) * 2 if count_str else 8  # æ¯è½®2æ¡æ¶ˆæ¯(user+assistant)
            text = memory.to_text(max_msgs=max_msgs) or "(ç©º)"
            vmem_notes.add_memories([text], [{"type": "user_note", "name": name}])
            print(f"âœ… å·²ä¿å­˜æœ€è¿‘ {max_msgs//2} è½®å¯¹è¯ä¸ºè®°å¿†ï¼š{name}") 
            continue

        # 4) list memories
        if user_input.lower().startswith("list memories"):
            hits = vmem_notes.query("", k=20)  # ç©ºæŸ¥è¯¢æ—¶ peekï¼ŒVectorMemory.query å·²åšä¿æŠ¤
            if not hits:
                print("ï¼ˆæš‚æ— å‘½åè®°å¿†ï¼‰")
            else:
                print("\nğŸ—‚ å·²ä¿å­˜è®°å¿†ï¼ˆTop 20ï¼‰ï¼š")
                for i, h in enumerate(hits, 1):
                    nm = h.get("meta", {}).get("name") or "(æœªå‘½å)"
                    print(f"{i}. {nm}  score={h['score']:.2f}")
            continue

        # 5) delete åç§°
        if user_input.lower().startswith("delete "):
            name = user_input.split(" ", 1)[1].strip()
            cand = vmem_notes.query(name, k=1)
            if not cand:
                print("âŒ æœªæ‰¾åˆ°å¯åˆ é™¤çš„è®°å¿†ã€‚"); continue
            _id = cand[0]["id"]
            # Chroma Python å®¢æˆ·ç«¯æ”¯æŒ delete(ids=[...])ï¼ˆä½ çš„ VectorMemory æ²¡å°è£…ï¼Œç›´æ¥ç”¨åº•å±‚ï¼‰
            vmem_notes.col.delete(ids=[_id])
            print(f"ğŸ—‘ï¸ å·²åˆ é™¤ï¼š{cand[0].get('meta',{}).get('name','(æœªå‘½å)')}"); continue

        # 6) recall åç§°/å…³é”®è¯ â€”â€” å¬å›å¹¶æ³¨å…¥ä¸Šä¸‹æ–‡
        if user_input.lower().startswith("recall "):
            key = user_input.split(" ", 1)[1].strip()
            hits = vmem_notes.query(key, k=5)
            if not hits:
                print("âŒ æœªæ‰¾åˆ°ç›¸å…³è®°å¿†ã€‚"); continue
            _, block = build_recalled_context(hits, min_score=0.0, max_items=5, label="note")
            # ç›´æ¥æŠŠå¬å›çš„è®°å¿†ä½œä¸º system ä¸Šä¸‹æ–‡æ³¨å…¥ï¼ˆä¸€æ¬¡æ€§æœ‰æ•ˆï¼‰
            memory.add("system", "ã€å¬å›è®°å¿†ã€‘\n" + block)
            print("ğŸ” å·²å°†è®°å¿†æ³¨å…¥ä¸Šä¸‹æ–‡ï¼Œæœ¬è½®å›ç­”ä¼šå‚è€ƒä»¥ä¸Šå†…å®¹ã€‚"); 
            # ä¸ç»§ç»­ï¼Œå› ä¸ºè¿˜è¦è®©ç”¨æˆ·ä¸‹ä¸€æ¡é—®é—®é¢˜
            continue

        # ---------- å¸¸è§„å¯¹è¯ï¼šå…ˆåš RAG å¬å›ï¼Œå†é—®ç­” ----------
        kd, kf, kn, recalled_block = query_all(
            vmem_docs, vmem_facts, vmem_notes, user_input, k_each=8, min_score=0.2
        )
        context_msg = ""
        if recalled_block:
            context_msg = ("ã€æ£€ç´¢åˆ°çš„ç›¸å…³èµ„æ–™ï¼ˆè¯·ä¼˜å…ˆä¾æ®è¿™äº›ç‰‡æ®µå›ç­”ï¼Œå¹¶åœ¨å¥æœ«æ ‡æ³¨ [R#] å¼•ç”¨ï¼›"
                           "docs=å¤–éƒ¨æ–‡æ¡£ï¼Œfacts=é•¿æœŸäº‹å®ï¼Œnote=ç”¨æˆ·è®°å¿†ï¼‰ã€‘\n" + recalled_block)

        # çŸ­æœŸè®°å¿†ï¼šå†™å…¥ç”¨æˆ·æ¶ˆæ¯
        memory.add("user", user_input)

        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "developer", "content": DEVELOPER_HINT},
        ]
        if context_msg:
            messages.append({"role": "system", "content": context_msg})
        messages += memory.get()

        try:
            resp = client.chat.completions.create(
                model=CHAT_MODEL, messages=messages, temperature=0.7
            )
            reply = resp.choices[0].message.content.strip()
        except Exception as e:
            print(f"âš ï¸ å‡ºé”™äº†: {e}"); continue

        print("Chatbotï¼š", reply, "\n")
        memory.add("assistant", reply)

if __name__ == "__main__":
    main()